# ğŸ§ ğŸ•¹ï¸ MazeMaster: A Dynamic Grid World + Deep Reinforcement Learning Playground

**Survival of the Smartest.**  
Welcome to **MazeMaster** â€“ a dynamic, obstacle-ridden grid world where pathfinding meets chaos. This isnâ€™t your average gameâ€”it's an evolving, AI-driven experiment testing the wit of agents trained through Q-learning and CNN . Whether youâ€™re here to build, play, or train the next grid-exploring super-agentâ€”**youâ€™ve found the right maze.**

---

## ğŸš€ Features at a Glance

ğŸ§± **Dynamic Obstacles**  
Walls rearrange every few moves. Strategy today is dead tomorrow.

ğŸ’‰ **Health Mechanics**  
Bump into walls? Itâ€™ll cost you. Watch your agent go from Full â†’ Injured â†’ Critical.

ğŸ¯ **Smart Goal Placement**  
Randomized targets ensure no two episodes feel the same.

ğŸ§  **Reinforcement Learning Agents**  
- ğŸ’¾ Classic **Q-learning** with partial observability
- ğŸ“¦ CNN-powered agent with PyTorch
- ğŸ” Local state hashing and multi-channel observations

ğŸ‘€ **Visualization FTW**  
Pygame UI with animated bots, reward displays, particle effects & retro vibes.

---

## ğŸ§ª Core Modules

| Module | Purpose |
|--------|---------|
| `mdp_gym.py` | Custom Gym environment with health, wall logic, rewards |
| `vis_gym.py` | Pygame-powered environment visualization |
| `MFMC.py` | Q-learning agent with partial observation support |
| `CNN.py` | DQN agent with CNN architecture and experience replay |
| `dynenv.py` | Variant environment with red/orange penalty zones |
| `environment.py` | Early prototype with hard/soft wall logic |

---

## ğŸ§  Agent Intelligence

### âœ… Q-Learning (MFMC)
- Local 3x3 grid hashed into stable state representation
- Epsilon-decay exploration
- Wall-aware decision making

### ğŸ§  DQN (CNN.py)
- 3-channel grid: walls, player, goal
- CNN model trained via replay buffer & target network
- Full TensorBoard logging and model checkpoints

---

## ğŸ•¹ï¸ How to Play


# For manual play (WASD controls)
python vis_gym.py



# Run Q-learning
python MFMC.py


# Train the DQN agent
python CNN.py

---

## ğŸ’¡ Nerdy Nuggets

- Grid size is customizable (currently 8x8)
- Wall density ~30%
- Partial observability via 3x3/5x5 windows
- DQN supports both CPU, MPS, and CUDA backends
- Particle celebration on goal reach ğŸ˜

---

## ğŸ“‚ Project Tree


.
â”œâ”€â”€ mdp_gym.py       # Core Gym environment
â”œâ”€â”€ vis_gym.py       # Game visualization
â”œâ”€â”€ MFMC.py          # Q-learning loop
â”œâ”€â”€ CNN.py           # Deep Q-Learning with PyTorch
â”œâ”€â”€ dynenv.py        # Alternate env with refreshable obstacles
â”œâ”€â”€ environment.py   # Legacy version with hard/soft wall rules
â”œâ”€â”€ Q_table.pickle   # Trained Q-table (generated)
â””â”€â”€ .gitignore




## ğŸ“ˆ Result Demos

| ğŸ® Episode | Reward Curve | Action Map |
|-----------|---------------|------------|
| ![goal](https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExdDkwMHN3b2dmc3ZxbjhzYTZ5emY4amtlczR6eWZyaHczbTgyNnM1dCZlcD12MV9naWZzX3NlYXJjaCZjdD1n/Ge86Xwnb0FGXmT4D6o/giphy.gif) | *(TensorBoard logs available)* | *(Coming Soon)* |

---

## ğŸ“š Credits & Nerds Behind the Code

Crafted by a team obsessed with intelligent agents, reward hacks, and retro-style game design.  
Drop us a ğŸŒŸ if you love robots learning how to escape unpredictable mazes.

